{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.database_manager import SQLTableManager\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from io import StringIO\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "from src.custom_logger import CustomLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiliazing the Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[scraper] INFO (05-10 12:31 PM): ################## Logging Started ################## (Line: 8) [865591240.py]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of CustomLogger with logger name and log directory\n",
    "logger_instance = CustomLogger(\"scraper\",\"scraper\",r\"C:\\Users\\Apoorva.Saxena\\OneDrive - Sitio Royalties\\Desktop\\Project - Apoorva\\Python\\Scraping\\RRC\\src\\logs\")\n",
    "\n",
    "# Get the logger\n",
    "logger = logger_instance.get_logger()\n",
    "\n",
    "# Start the logger\n",
    "logger.info(f\"################## Logging Started ##################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining URL's\n",
    "base_url = 'https://webapps2.rrc.texas.gov/EWA/'\n",
    "query_url = base_url + 'drillingPermitsQueryAction.do'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dt(date_str:str) -> datetime.date:\n",
    "    \"\"\"\n",
    "    date_str: YYYY-MM-DD\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return datetime.strptime(date_str, '%Y-%m-%d').date()\n",
    "    except ValueError:\n",
    "       logger.error(f\"Incorrect data format {date_str}. Should be YYYY-MM-DD\")\n",
    "\n",
    "       \n",
    "def get_APIs_from_SQL(date:datetime.date) -> pd.DataFrame:\n",
    "\n",
    "    # Create an instance of the class\n",
    "    sql_connector = SQLTableManager()\n",
    "\n",
    "    try:\n",
    "        sql_connector.connect()\n",
    "        \n",
    "        query = f'''SELECT distinct [API_10] FROM [Sitio_GIS].[dbo].[STR_WELL_UNITS]\n",
    "                    where ([API_10] like '42%') and ([Sitio_GIS].[dbo].[STR_WELL_UNITS].[created_date] >= '{date}');'''\n",
    "        \n",
    "        df = sql_connector.execute_query(sql_query=query)\n",
    "\n",
    "        logger.info(f\"Total API Count: {df.shape[0]}\")\n",
    "\n",
    "        logger.debug(\n",
    "            f\"API DataFrame Top 5 Rows:\\n {df.head()}\\n----------\\n\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error occured while reading {query} from server MsSQL server. Error details:\",exc_info=True)\n",
    "\n",
    "    finally:\n",
    "        sql_connector.close_connection()\n",
    "\n",
    "\n",
    "def get_headers() -> dict:\n",
    "\n",
    "    headers = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Host': 'webapps2.rrc.texas.gov',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'none',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "    'sec-ch-ua': \"Google Chrome;v='119', 'Chromium';v='119', 'Not?A_Brand';v='24'\",\n",
    "    'sec-ch-ua-mobile': \"?0\",\n",
    "    'sec-ch-ua-platform': \"Windows\"\n",
    "    }\n",
    "    \n",
    "    return headers\n",
    "\n",
    "\n",
    "def payload(api:str) -> dict:\n",
    "         \n",
    "    \"\"\"      \n",
    "    Parameters\n",
    "    ----------\n",
    "    api : TYPE, int\n",
    "        DESCRIPTION. 10 digit API. Strips API of initial '42'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check the length of API\n",
    "\n",
    "    try:\n",
    "        assert len(str(api)) == 14 or len(str(api)) == 10, f\"Length of API: {api} don't match with 10 or 14 digit\"\n",
    "        \n",
    "        if len(str(api)) == 14:\n",
    "            api = (str(api)[2:10])\n",
    "        \n",
    "        elif len(str(api)) == 10:\n",
    "            api = (str(api)[2:])\n",
    "\n",
    "    except AssertionError as e:\n",
    "        logger.error(e)\n",
    "\n",
    "    payload = {\n",
    "    'methodToCall': 'search',\n",
    "    'searchArgs.apiNoHndlr.inputValue': api,\n",
    "    'searchArgs.operatorNameWildcardHndlr.inputValue': 'beginsWith',\n",
    "    'searchArgs.leaseNameWildcardHndlr.inputValue': 'beginsWith',\n",
    "    'searchArgs.fieldNameWildcardHndlr.inputValue': 'beginsWith',\n",
    "    'searchArgs.surveyNameWildcardHndlr.inputValue': 'beginsWith'\n",
    "    }\n",
    "\n",
    "    return payload\n",
    "\n",
    "\n",
    "def get_data_DrillingPermitQuery(req_url:str, headers:dict, payload:dict) -> requests.Response:\n",
    "    \"\"\"      \n",
    "    Parameters\n",
    "    ----------\n",
    "    post_response : TYPE, Requests Response\n",
    "        DESCRIPTION. .\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Request Response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with requests.session() as s:\n",
    "            s.verify = False\n",
    "            res_get = s.get(url=req_url, headers=headers, verify=False)\n",
    "\n",
    "            if res_get.status_code == 200:\n",
    "                cookies = res_get.cookies\n",
    "                res_post = s.post(url=query_url, data=payload, cookies=cookies, headers=headers, verify=False)\n",
    "\n",
    "                return res_post\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error occured while requesting {req_url}. Error details:\",exc_info=True)\n",
    "\n",
    "\n",
    "def parse_query_data(post_resp:requests.Response) -> tuple[pd.DataFrame,pd.DataFrame,str,datetime.date]:\n",
    "    \"\"\"      \n",
    "    Parameters\n",
    "    ----------\n",
    "    post_response : TYPE, Requests Response\n",
    "        DESCRIPTION. Response from the get_data_DrillingPermitQuery()\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(post_resp.content,'html.parser')\n",
    "    web_links = soup.select('a')\n",
    "    href = [base_url + web_link['href'] for web_link in web_links if 'drillingPermitDetailAction' in web_link['href']]\n",
    "\n",
    "    df_main = pd.read_html(StringIO(post_resp.text))[9]\n",
    "    df_main.columns = df_main.iloc[1,:]\n",
    "    df_main = df_main.drop([0,1]).copy()\n",
    "    df_main = df_main.reset_index(drop=True).copy()\n",
    "\n",
    "    df_edit = df_main.copy()\n",
    "    df_edit['API NO.'] = df_main['API NO.'].str.split()[0][0]\n",
    "    df_edit['HREF'] = href\n",
    "    df_edit[['Submitted_Dt', 'Approved_Dt']] = df_edit['Status Date'].str.split(expand=True).loc[:,[1,3]]\n",
    "    df_edit['Submitted_Dt'] = pd.to_datetime(df_edit['Submitted_Dt'])\n",
    "    df_edit['Approved_Dt'] = pd.to_datetime(df_edit['Approved_Dt'])\n",
    "\n",
    "    apprvd_Dt_max_idx = df_edit.index[df_edit['Approved_Dt']==df_edit['Approved_Dt'].max()]\n",
    "    max_apprvd_Dt = df_edit['Approved_Dt'].max()\n",
    "    href_link = df_edit.loc[apprvd_Dt_max_idx,'HREF'].item()\n",
    "\n",
    "    return df_main, df_edit, href_link, max_apprvd_Dt, apprvd_Dt_max_idx.item()\n",
    "\n",
    "\n",
    "def get_data_W1_Form(url_href:str) -> requests.Response:\n",
    "    \"\"\"      \n",
    "    Parameters\n",
    "    ----------\n",
    "    url_href : TYPE, Str\n",
    "        DESCRIPTION. URL from parse_query_data()\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Response.\n",
    "    \"\"\"\n",
    "\n",
    "    with requests.session() as s:\n",
    "        s.verify = False\n",
    "        res_get = s.get(url=url_href,headers=get_headers(),verify=False)\n",
    "    \n",
    "    return res_get\n",
    "\n",
    "\n",
    "def parse_W1_Form(get_resp:requests.Response) -> pd.DataFrame:\n",
    "    \"\"\"      \n",
    "    Parameters\n",
    "    ----------\n",
    "    get_resp : TYPE, Requests Response\n",
    "        DESCRIPTION. get response from get_data_W1_Form()\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(get_resp.text,'html.parser')\n",
    "    tables = soup.find_all('table',class_='GroupBox1')\n",
    "\n",
    "    data_dict = {}\n",
    "\n",
    "    for count,table in enumerate(tables):\n",
    "        if (table.find_all('th') is not None):\n",
    "            for i,(th,td) in enumerate(zip(table.find_all('th'),table.find_all('td'))):\n",
    "                if th.text == 'Horizontal Wellbore':\n",
    "                    data_dict[th.text] = td.text\n",
    "                elif th.text == 'Acres':\n",
    "                    data_dict[th.text] = td.text\n",
    "\n",
    "    return pd.DataFrame.from_dict(data_dict, orient='index').T\n",
    "\n",
    "\n",
    "def formatting_results_df(dataframe:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Formatting dfs_unique_Results DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Formatting the columns\n",
    "    dataframe.columns = dataframe.columns.str.lower().str.replace(\".\",\"\").str.replace(\"#\",\"\").str.strip().str.replace(\" \",\"_\")\n",
    "\n",
    "    # Keeping relevant columns\n",
    "    dataframe = dataframe[['api_no', 'lease', 'well_number', 'filing_purpose', 'amend', \n",
    "                           'status', 'submitted_dt','approved_dt', 'horizontal_wellbore', 'acres','href']].copy()\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def results_to_db(dataframe:pd.DataFrame, sqlTableName:str='well_search_results') -> None:\n",
    "\n",
    "    manager = SQLTableManager()\n",
    "\n",
    "    try:\n",
    "        manager.connect(dbname='Sandbox_Engineering')\n",
    "        manager.add_rows_from_dataframe(sql_table_name=sqlTableName, dataframe=dataframe)\n",
    "        logger.info(f\"Added rows to {sqlTableName} \\n{dataframe}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        \n",
    "        # If SQL insert fails, create csv file in the working directory\n",
    "        now = datetime.now().strftime('%m_%d_%Y-%H_%M_%S')\n",
    "        filename: str = f'RRC_Scraper_Results_{now}.csv'\n",
    "        dataframe.to_csv(filename, index=False)\n",
    "        \n",
    "        logger.error(f\"Failed to add rows to {sqlTableName}. Created the {filename} file. Error details:\",exc_info=True)\n",
    "    finally:\n",
    "        manager.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dt:str) -> None:\n",
    "    \"\"\"\n",
    "    dt: Date Input Format YYYY-MM-DD\n",
    "    \"\"\"\n",
    "\n",
    "    # Get API's from STR_WELL_UNITS DB >= Date\n",
    "    date_filter = convert_dt(dt)\n",
    "\n",
    "    # Running Query and getting the API's into DataFrame df_api\n",
    "    df_api = get_APIs_from_SQL(date_filter)\n",
    "    \n",
    "    logger.info(f\"Running the scraper on {df_api.shape[0]} unique wells.\")\n",
    "\n",
    "    # Defining empty lists\n",
    "    dfs_main = []\n",
    "    dfs_edit = []\n",
    "    dfs_unique_Results = []\n",
    "\n",
    "    # Define Headers\n",
    "    headers = get_headers()\n",
    "\n",
    "    # Running the loop\n",
    "    for idx, api in enumerate(df_api['API_10']):\n",
    "\n",
    "        payload_api = payload(api=api)\n",
    "\n",
    "        if idx > 1:\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        response_from_DrillingPermit_Query = get_data_DrillingPermitQuery(req_url=query_url, headers=headers, payload=payload_api)\n",
    "\n",
    "        df_mainPage, df_edit, w1_link, Approved_Dt_max, idx_Max_Aprvd_Dt = parse_query_data(post_resp = response_from_DrillingPermit_Query)\n",
    "\n",
    "        if idx > 1:\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        response_from_W1_query = get_data_W1_Form(url_href=w1_link)\n",
    "\n",
    "        df_w1 = parse_W1_Form(get_resp=response_from_W1_query)\n",
    "        df_w1['Approved_Dt'] = Approved_Dt_max\n",
    "\n",
    "        df_edit_merge_with_w1 = df_edit.merge(df_w1,on='Approved_Dt',how='left').reset_index(drop=True)\n",
    "\n",
    "        df_unique_res = df_edit_merge_with_w1[df_edit_merge_with_w1['Approved_Dt']==Approved_Dt_max].reset_index(drop=True).copy()\n",
    "\n",
    "        dfs_edit.append(df_edit_merge_with_w1)\n",
    "        dfs_main.append(df_mainPage)\n",
    "        dfs_unique_Results.append(df_unique_res)\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        if idx==5:\n",
    "            break\n",
    "\n",
    "    logger.info(f\"All {df_api.shape[0]} wells got succesfully scraped!\")\n",
    "\n",
    "\n",
    "    # Formating unique results dataframe's columns\n",
    "    df_Results = formatting_results_df(dataframe=pd.concat(dfs_unique_Results).reset_index(drop=True))\n",
    "\n",
    "    # Pushing data to SQL\n",
    "    logger.info(\"Pushing scraper results to SQL!\")\n",
    "    results_to_db(dataframe=df_Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\apoorva.saxena\\onedrive - sitio royalties\\desktop\\project - apoorva\\python\\scraping\\rrc\\src\\database_manager.py:37: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  result_df = pd.read_sql(sql_query, self.connection)\n",
      "[scraper] INFO (05-10 12:31 PM): Total API Count: 130 (Line: 24) [2753276606.py]\n",
      "\n",
      "[scraper] INFO (05-10 12:31 PM): Running the scraper on 130 unique wells. (Line: 12) [2910409013.py]\n",
      "\n",
      "[scraper] INFO (05-10 12:31 PM): All 130 wells got succesfully scraped! (Line: 55) [2910409013.py]\n",
      "\n",
      "[scraper] INFO (05-10 12:31 PM): Pushing scraper results to SQL! (Line: 62) [2910409013.py]\n",
      "\n",
      "[scraper] INFO (05-10 12:31 PM): Added rows to well_search_results \n",
      "     api_no                 lease well_number filing_purpose amend    status  \\\n",
      "0  12335385             WILLEKE A         21H      New Drill     N  APPROVED   \n",
      "1  12335388       LESKE-MOTL SA A          1H      New Drill     N  APPROVED   \n",
      "2  12335389       LESKE-MOTL SA B          2H      New Drill     N  APPROVED   \n",
      "3  12335390       LESKE-MOTL SA C          3H      New Drill     N  APPROVED   \n",
      "4  12335391  LESKE-JUDD-MOTL SA D          4H      New Drill     N  APPROVED   \n",
      "5  12335392          P. FRISBIE B          6H      New Drill     N  APPROVED   \n",
      "\n",
      "  submitted_dt approved_dt horizontal_wellbore    acres  \\\n",
      "0   2024-02-15  2024-03-21                       572.71   \n",
      "1   2024-03-07  2024-03-22          Allocation  1048.77   \n",
      "2   2024-03-07  2024-03-22          Allocation  1048.77   \n",
      "3   2024-03-07  2024-03-22          Allocation  1048.77   \n",
      "4   2024-03-07  2024-03-22          Allocation  1428.54   \n",
      "5   2024-03-19  2024-03-28                        320.0   \n",
      "\n",
      "                                                href  \n",
      "0  https://webapps2.rrc.texas.gov/EWA/drillingPer...  \n",
      "1  https://webapps2.rrc.texas.gov/EWA/drillingPer...  \n",
      "2  https://webapps2.rrc.texas.gov/EWA/drillingPer...  \n",
      "3  https://webapps2.rrc.texas.gov/EWA/drillingPer...  \n",
      "4  https://webapps2.rrc.texas.gov/EWA/drillingPer...  \n",
      "5  https://webapps2.rrc.texas.gov/EWA/drillingPer...   (Line: 227) [2753276606.py]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Running Main Program (Change the date from where you want the grab the APIs from)\n",
    "    main('2024-04-15')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
